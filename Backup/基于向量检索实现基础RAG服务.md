# 基于向量检索实现基础RAG服务
`RAG Retrieval-Augmented Generation`是一种用于自然语言处理的模型架构，结合了检索`Retrieval`和生成`Generation`两种技术。而`RAG`服务在知识问答、代码生成、事实验证、专业领域检索等任务中表现出色，能够通过检索相关知识来增强生成模型的回答质量和准确性。

实际上，当前`RAG`相关建设已经比较成熟，目前看起来其实并不太赶得上潮流，但学习`RAG`最好的时间是`22`年底，其次是现在。`RAG`服务和当前的`AI Infra`建设有着密切的关系，作为基础建设的`RAG`是一个以搜索为核心，围绕各种数据、知识、`LLMs`等服务协作运行的复杂系统。

## 描述
实际上我们当前聊的主要是`RAG`中的`RA`部分，这部分还是比较偏向于传统的`NLP`任务，本文中主要涉及的是文本的向量化和向量检索。而在`G`这部分则结合了检索和生成的框架，主要用于增强生成模型的能力，并且能够根据提问以及召回内容提高生成文本的质量和准确性。

向量检索是`RAG`服务的核心部分，但本质上的主要目标是内容检索，因此我们并非仅限于使用向量方法来检索内容。基于图数据库的`GraphRAG`，或者是`ElasticSearch`基于倒排索引的全文检索能力，都是可以来实现`RAG`的检索服务的，这样我们可以将相关检索方法来统一召回并排名。

此外，我们在使用`LLMs`的时候，是比较难以获取最新的知识的，毕竟`GPT`实际上是生成式预训练模型的缩写，既然是预训练模型自然是不能实时进行训练获取最新知识的。而如果我们使用`RL`强化学习、`SFT`监督微调等方式来进行微调训练，需要更高的标注内容以及计算资源等，成本比较高。

因此，`RAG`服务是较简单且成本低的模式，以此来提供给`LLMs`输入`Context`，而实际上我们调优`Prompt`的方式也可以算作是提供`Context`的一种方式，当然诸如`Function Call`、`MCP`也可以作为给予`LLMs`上下文的方式。而简单来说，`RAG`服务具有以下的使用场景:

- 需要获取较新的知识内容: 当需要查询特定领域的最新研究进展、或者企业内部实时更新的文档，包括我们常用的联网搜索场景，也可以认为是特定形式的`RAG`。当然如果需要获取最新的相关情况，还需要结合`Function Call`的方式，例如查询实时天气等。
- 需要提供特定领域的知识: 当需要查询特定领域的专业知识，例如医学、法律等领域的专业文档，或者是企业内部的知识库内容，这些内容通常是专业、私有、非公开或未包含在`LLMs`通用训练数据中的知识，因此可以结合`RAG`服务来提供相关内容。
- 增强透明性与可解释性: 在需要审核、溯源或理解`LLMs`决策依据的场景，例如金融分析报告、法律建议初稿、医疗信息查询等。`RAG`系统可以同时返回生成答案和其依据的检索来源，这为用户提供了验证答案正确性的途径，也增加了系统输出的透明度和可信度。
- 数据长尾分布的检索: 当数据分布呈现长尾形态时，通用`LLMs`可能因训练数据覆盖不足而无法给出满意答案。`RAG`能够通过检索覆盖到稀有或少见案例，提升模型在这些长尾数据上的表现，当然这本身也会比较依赖于`RAG`服务本身的检索能力。
- 垂直搜索与智能问答: 针对特定领域或企业内容的智能搜索和问答机器人，`RAG`天然适合此类场景。用户问题触发对专属知识库的检索，检索到的相关内容被用于生成精准、简洁、符合上下文的答案，提供比传统关键词匹配更自然、信息量更大。

本文实现了非常基础的`RAG`示例，`Embedding`使用轻量的`all-MiniLM-L6-v2`模型，向量检索的数据库使用轻量的`hnswlib-node`实现，以此可以在`node`中直接运行起来 <https://github.com/WindRunnerMax/webpack-simple-environment/tree/master/packages/hnsw-rag>。

实际上当前很多云服务商以及开源项目提供了开箱即用`RAG`服务的实现，而我们自然可以根据需求来决定是否可以接入开箱即用的服务。只是若是我们需要精细地调配很多功能，例如自定义分片策略等，就比较依赖服务是否暴露相关实现，因此我们不一定可以直接处理。

我们通常都会明确专业的人做专业的事，开箱即用并没有什么问题。但是如果之前看过我的 [从零实现富文本编辑器](https://github.com/WindRunnerMax/EveryDay/blob/master/RichText/从零设计实现富文本编辑器.md) 系列文章的话，就可能会感觉出来，越依赖浏览器的能力就越需要处理默认行为带来的复杂`Case`，我们使用开箱即用的服务也同样如此。如果自定义场景要求比较高，那么就会面临不受控的情况。

所以在这篇文章中我们实现了简单的`RAG`服务，也可以从侧面反映出来我们能够在`RAG`服务上做些什么精调的方案，来提高召回率和召回效果。在知道了我们可以对服务做哪些方面的参数和数据调整后，才能够比较有针对性地进行优化。

## 文本向量化
先前已经提到了，我们在这里主要的侧重点还是倾向传统的`NLP`检索任务，或者我们可以说是文本的搜索任务。因此简化一下我们需要做的事情就是 预处理-编码-存储-检索 这四部分，在本节中我们主要介绍文本的预处理和编码方式。

### 数据分片
在数据预处理环节，我们主要需要做的是数据清洗以及分片，数据清洗部分主要是去除文本内容中的噪音、重复内容、过滤低信息量文本等等，这部分的策略可以根据具体的业务需求来进行调整。而分片则是将较大的文章切分为较小的片段，以便于后续的向量化和检索。

我们在这里主要讨论的是分片方式，分片的方式解决了两个问题，一是文档的内容会很长，若是直接将文档内容作为整体进行向量化，可能会导致向量的维度过高，计算和存储成本较大。二是文档内容的语义信息可能会分布在不同的段落中，大部分内容可能与用户问题无关，整体向量化后召回效果可能欠佳。

实际上还有个额外的原因，通过这种方式我们可以避免整篇文档被召回，这样可以省去不少的`Token`消耗。并且长上下文给予`LLMs`时，最开始的内容可能还是会被遗忘。因此分片是比较公认的处理方案，分而治之的思想在这里就很合适，并且分片的方式我们可以去精调，来提高召回率以及召回效果。

接下来我们可以考虑分片的方式，常见的分片方式有 固定大小分片、基于句段分片、基于结构分片 等方式。在这里我们以下面的一段文本为例，简单介绍一下分片的方式。需要注意的是，由于我们是为了演示用的，分片切的比较小，而实际上分片通常选的是`512/1024/4096`等长度的片段。

```
## 深度学习
深度学习是机器学习的一个子集，使用多层神经网络来模拟人脑处理信息的方式。它在图像识别、语音识别和自然语言处理等领域取得了显著进展。
```

最基本的分片方式是按照固定的长度分片，即固定一个最大长度，然后根据这个长度值直接切分。这种简单粗暴的方式可以快速实现原型，但可能会导致语义信息的丢失。例如，若我们将上面的文本按照每个分片最大长度为`30`字符进行分片，得到的结果是:

```js
[
  "## 深度学习\n深度学习是机器学习的一个子集，使用多层神经网",
  "络来模拟人脑处理信息的方式。它在图像识别、语音识别和自然语言",
  "处理等领域取得了显著进展。"
]
```

虽然文本看起来非常规整，但实际上语义信息被切割得比较碎片化，而且部分片段可能包含填充或冗余信息，导致检索精度降低。而处理这个问题比较常见的思路是采用`overlap`重叠分片的方式，即在每个分片之间保留一定的重叠部分，以便于保留更多的上下文信息，我们在上述基础上重叠`5`个字符:

```js
[
  "## 深度学习\n深度学习是机器学习的一个子集，使用多层神经网络来模拟人",
  "多层神经网络来模拟人脑处理信息的方式。它在图像识别、语音识别和自然语言处理等领域",
  "和自然语言处理等领域取得了显著进展。"
]
```

虽然看起来是好了不少，但实际上这种方式仍然会导致语义信息的丢失，尤其是当分片长度较短时，可能会导致上下文信息的缺失，而上述提到的分片长度比较大的情况下是表现相对好一些的。在本文的示例中，就是采用的这种简单的分片方式来进行分片的。

```js
/**
 * 文本分片方法
 */
const spiltTextChunks = (text: string): string[] => {
  // 这里是直接根据 固定长度 + overlap 分片，此外还有 结构分片 等策略
  const chunkSize = 100;
  const overlap = 20;
  const chunks: string[] = [];
  for (let i = 0; i < text.length; i += chunkSize - overlap) {
    const chunk = text.slice(i, i + chunkSize);
    if (chunk.trim().length > 0) {
      chunks.push(chunk.trim());
    }
  }
  return chunks;
};
```

第二种分片的方式是基于句段分片，即根据文本中的句子或段落进行分片。这种方式可以更好地保留语义信息，因为这样通常会在句子或段落的边界进行切分，而不是任意长度的字符切分。例如，我们可以将上面的文本按照句子进行分片，得到的结果是:

```js
[
  "## 深度学习",
  "深度学习是机器学习的一个子集，使用多层神经网络来模拟人脑处理信息的方式。",
  "它在图像识别、语音识别和自然语言处理等领域取得了显著进展。"
]
```

这种方式看起来会好很多，语义信息保留得比较完整，避免中间分割，确保片段包含连贯思想，有效维护文档的原始流程和上下文完整性。但是实际上还是面临一些问题，例如切出的句子长度太短，或者是切出的句子太长超出了我们的`Max Token`限制等问题。

因此我们也需要针对其实际表现来额外增加一些策略，例如较短句子中我们可以不断拼接后续的内容，直至其逼近`Max Token`限制。而较长的句子我们可以采用固定分片方式进行分割处理，来确保每个片段都在合理的长度范围内，这样可以更好地平衡语义信息的完整性和片段长度的限制。


```js
[
  "## 深度学习\n深度学习是机器学习的一个子集，使用多层神经网络来模拟人脑处理信息的方式。它在图像识别、语音识别和自然语言处理等领域取得了显著进展。"
]
```

第三种分片方式是基于结构分片，即根据文本的结构进行分片，例如标题、段落、列表等，特别是我们给予模型的输入通常都是`Markdown`格式的文本，因此其天然就具有一定的结构化信息。基于结构分片可以更好地保留文本的层次和逻辑关系，避免语义信息的丢失。

```js
[
  "- 列表1\n- 列表2\n- 列表3",
  "## 深度学习\n深度学习是机器学习的一个子集，使用多层神经网络来模拟人脑处理信息的方式。它在图像识别、语音识别和自然语言处理等领域取得了显著进展。"
]
```

实际上，当前并没有一种普遍适用的最佳分片策略，分片的复杂点在于确定片段的最佳大小和结构。如果片段过小，它们可能会丢失关键的上下文信息，从而变得不那么有意义。相反如果片段过大，检索过程的效率会降低，并且可能会无意中包含不相关的信息，从而稀释结果的相关性。

除了上述策略，分片时还需考虑其他重要因素，理想情况下，片段应保留语义单元，例如完整的句子、段落或主题，以确保连贯性并防止意义中断。采用重叠片段或者滑动窗口其实还是个比较好的方案，有助于在片段边界处保持上下文，确保关键信息或过渡上下文不会在片段之间丢失。

此外，我们甚至还可以让`LLMs`来帮助我们进行分片，语言模型其实更擅长理解和处理自然语言文本，这也是一种基于语义分片的策略。然而这种方式在处理大量数据的时候就需要考虑效率以及计算资源问题了，而且`LLMs`也并非银弹，具体效果还是需要构建评测集合来验证。

还有一个比较重要的点是，我们在分片的时候是可以并入元信息的，这点是很容易被忽视的。元信息可以是文档的标题、作者、发布时间等，这些信息可以帮助模型更好地理解片段的上下文和背景，甚至如果我们有文档相关的人工打标信息，会更好地帮助我们进行重排以及内容生成。

### 编码方式
在数据预处理完成后，我们就需要考虑数据存储的方式了。在这里我们首先需要考虑一个问题，计算机是否能直接处理文本，答案自然是否定的。计算机本质上是只能处理数字的，因此我们需要将文本转换为计算机可以理解的数字形式，这个过程称为编码。

那么我们考虑字体的这种形式，如果我们直接将文本处理成`UTF-8`编码的字节流的话，是不是就可以直接存储了。在这种情况下存储自然是可以直接存储的，但检索的时候就会比较麻烦，我们希望的是实现语义搜索，需要理解词语和短语背后的含义和意图，而`UTF-8`自然不能直接提供语义信息。

此外，`NLP`常用的`Token`代表的是词组而非字也是类似的考虑，单词通常包含更丰富的语义信息，而单个字符通常没有这样的语义。例如，苹果这个词有明确的含义，而单个的字符苹和果分别并不能传达完整的语义。且分词可以减少文本序列的长度，降低复杂度和计算量。

而在编码的实现中，我们通常都会使用向量来表示词组，那么最简单的向量化方式是`One-Hot`编码方式。即将每个词组表示为一个高维稀疏向量，其中只有一个元素为`1`，其余元素为`0`。这种方式简单直观，但存在维度灾难和语义信息稀疏的问题。

```
自然   1   0   0   0   [1, 0, 0, 0]  
语言   0   1   0   0   [0, 1, 0, 0]
处理   0   0   1   0   [0, 0, 1, 0]
任务   0   0   0   1   [0, 0, 0, 1]
```

类似维度都是词汇表大小的还有`TF-IDF`编码方式，即词频-逆文档频率​。`TF`词频，一个词在单篇文档中出现的次数越多，可能越重要。​`IDF`逆文档频率`，如果一个词在所有文档中出现得越普遍比如停用词，就越不重要。同样会导致产生非常高的纬度，导致向量语义信息稀疏。

​`TF-IDF`编码主要应用的方向是文档转化为结构化、数值化的向量表示，便于进行文本分类、聚类、相似度计算等任务。是通过词统计的形式来处理文档级的特征信息，而不是单个词的语义信息，因此实际上跟`One-Hot`编码的目标是不一样的，这里主要是举例高纬稀疏向量的问题。

```
文档	自然	 语言	 处理	  任务
D1	   0.223   0.511   0     	0
D2     0.223   0	   0.916	0.121
D3	   0.223   0.511   0.916	0
```

而在`NLP`的词向量生成的发展中，`Google`提出的`Word2Vec`模型是一个重要的里程碑。通过神经网络模型，将文本中的单词映射到高维向量空间，使得语义相似的词在该空间中的距离较近。这个算法可以产生稠密向量，并且捕捉到词语间的语义关系。

其中两种主要的训练方法是`CBOW`和`Skip-Gram`。`CBOW`通过上下文预测中心词，类似于完形填空，而`Skip-Gram`则是反过来，通过中心词预测上下文。这两种方法在训练事，就能有效地联系到词语的语义信息，并且生成的向量维度通常较低(稠密)，能够更好地表示词语之间的关系。

```
 dog                            faster
    \                         /        \
    dogs     cats        fast   slower  fastest
      \      /                 /      \
       animals              slow      slowest
```

`Word2Vec`本身已经非常成熟，但是其本身还是存在一定的局限性。其词向量的意思固定，每个词只有一个向量，无法表达多义词，例如苹果公司与水果苹果。其次，忽视了词语顺序，直接把句子看成词的集合，忽略了词序，例如猫追狗和狗追猫的向量一样。

而在我们的`RAG`系统中，输入的基准都是句段，而不仅仅是简单的词汇集合，因此使用`Word2Vec`的方式还不够。由此`Google`又提出了`Transformers`架构`Attention Is All You Need`，采用动态上下文编码的方式、长距离依赖捕捉等方式解决上述的问题。

```
I ate an [apple].             apple => [0.8, -0.2, 1.1]
I bought an [apple] phone.    apple => [1.5, 0.3, -0.7]
```

然而`Transformers`同样并非银弹，其计算复杂度较高，尤其是长文本的处理，因此在实际应用中会面临计算资源和时间成本的问题。此外，因为这种一词多义的表达，如果数据集不够大或者不够多样化，可能会导致模型无法很好地捕捉到词语的多义性和上下文信息，因此需要的语料是巨量的。

说句题外话，`GPT`和`BERT`都是基于`Transformer`实现，以当年我的理解来说，`BERT`无疑是更优秀的模型，其能够理解文本，后续只需要跟随一个解码器，例如全连接层就可以轻松实现诸如文本分类的任务。而由于`BERT`实际是文本编码，将其再接`GPT`作为解码器生成文本自然可行。

- 编码器: 编码器的主要任务是将输入序列转换为一种表示，这种表示能够捕捉输入序列的全局信息。
- 解码器: 解码器的主要任务是根据某种输入，可能是编码器的输出或之前的生成结果，逐步生成输出序列。
- `GPT`模型: `GPT`采用单向`Transformer`解码器，只能利用上文信息，适合生成任务。通过自回归语言模型预测下一个词，训练目标是最大化序列的联合概率，适合于文本生成类任务，文本翻译、生成式回答等。
- `BERT`模型: 使用双向`Transformer`编码器，能同时捕捉上下文信息，适合理解任务。通过`MLM`随机掩码部分词并预测，`NSP`判断两个句子是否连续，适合于理解分析类任务，文本分类、相似度比较等。

但是实际上目前看还是`GPT`成功了，还是以文本生成+`Prompt`这种看起来迂回式地完成任务，看起来更容易普及一些。生成式预训练的单向解码器，配合提示词可以完成多种任务，无需针对每个任务单独设计模型结构或训练方式。

回到我们的基础`RAG`服务，我们使用的是非常轻量的`all-MiniLM-L6-v2`模型来进行文本的向量化。具体来说是`INT8`量化的版本，因为我们的主要目标是跑`DEMO`，而不是追求最高的精度和性能，真正跑的话可以使用服务商提供的服务，例如`bge-m3`、`doubao-embedding`等模型。

```js
import { env, pipeline } from "@xenova/transformers";

// 初始化编码模型
const model = "Xenova/all-MiniLM-L6-v2";
env.localModelPath = path.resolve(__dirname, "../", "embedding");
const featureExtractor = await pipeline("feature-extraction", model);

/**
 * 文本 embedding
 */
const embeddingTextChunk = async (
  featureExtractor: FeatureExtractionPipeline,
  text: string
): Promise<number[]> => {
  const output = await featureExtractor(text, {
    pooling: "mean",
    normalize: true,
  });
  return Array.from(output.data);
};
```

## 向量检索

### 向量距离

### 混合召回

### 召回重排

## LLMs

### Query 改写

### 输入优化

### 生成增强

## 总结


如果`LLMs`存在足够长的输入，是否有必要使用`RAG`服务来补充`Context`信息。

`LangChain`


## 每日一题

- <https://github.com/WindRunnerMax/EveryDay>

## 参考
- <https://www.zhihu.com/question/653424464>
- <https://www.volcengine.com/docs/82379/1583857>
- <https://blog.langchain.com/evaluating-rag-pipelines-with-ragas-langsmith/>
