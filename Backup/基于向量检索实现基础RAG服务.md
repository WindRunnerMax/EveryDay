# 基于向量检索实现基础RAG服务
`RAG Retrieval-Augmented Generation`是一种用于自然语言处理的模型架构，结合了检索`Retrieval`和生成`Generation`两种技术。而`RAG`服务在知识问答、代码生成、事实验证、专业领域检索等任务中表现出色，能够通过检索相关知识来增强生成模型的回答质量和准确性。

实际上，当前`RAG`相关建设已经比较成熟，目前看起来其实并不太赶得上潮流，但学习`RAG`最好的时间是`22`年底，其次是现在。`RAG`服务和当前的`AI Infra`建设有着密切的关系，作为基础建设的`RAG`是一个以搜索为核心，围绕各种数据、知识、`LLMs`等服务协作运行的复杂系统。

## 描述
实际上我们当前聊的主要是`RAG`中的`RA`部分，这部分还是比较偏向于传统的`NLP`任务，本文中主要涉及的是文本的向量化和向量检索。而在`G`这部分则结合了检索和生成的框架，主要用于增强生成模型的能力，并且能够根据提问以及召回内容提高生成文本的质量和准确性。

向量检索是`RAG`服务的核心部分，但本质上的主要目标是内容检索，因此我们并非仅限于使用向量方法来检索内容。基于图数据库的`GraphRAG`，或者是`ElasticSearch`基于倒排索引的全文检索能力，都是可以来实现`RAG`的检索服务的，这样我们可以将相关检索方法来统一召回并排名。

此外，我们在使用`LLMs`的时候，是比较难以获取最新的知识的，毕竟`GPT`实际上是生成式预训练模型的缩写，既然是预训练模型自然是不能实时进行训练获取最新知识的。而如果我们使用`RL`强化学习、`SFT`监督微调等方式来进行微调训练，需要更高的标注内容以及计算资源等，成本比较高。

因此，`RAG`服务是较简单且成本低的模式，以此来提供给`LLMs`输入`Context`，而实际上我们调优`Prompt`的方式也可以算作是提供`Context`的一种方式，当然诸如`Function Call`、`MCP`也可以作为给予`LLMs`上下文的方式。而简单来说，`RAG`服务具有以下的使用场景:

- 需要获取较新的知识内容: 当需要查询特定领域的最新研究进展、或者企业内部实时更新的文档，包括我们常用的联网搜索场景，也可以认为是特定形式的`RAG`。当然如果需要获取最新的相关情况，还需要结合`Function Call`的方式，例如查询实时天气等。
- 需要提供特定领域的知识: 当需要查询特定领域的专业知识，例如医学、法律等领域的专业文档，或者是企业内部的知识库内容，这些内容通常是专业、私有、非公开或未包含在`LLMs`通用训练数据中的知识，因此可以结合`RAG`服务来提供相关内容。
- 增强透明性与可解释性: 在需要审核、溯源或理解`LLMs`决策依据的场景，例如金融分析报告、法律建议初稿、医疗信息查询等。`RAG`系统可以同时返回生成答案和其依据的检索来源，这为用户提供了验证答案正确性的途径，也增加了系统输出的透明度和可信度。
- 数据长尾分布的检索: 当数据分布呈现长尾形态时，通用`LLMs`可能因训练数据覆盖不足而无法给出满意答案。`RAG`能够通过检索覆盖到稀有或少见案例，提升模型在这些长尾数据上的表现，当然这本身也会比较依赖于`RAG`服务本身的检索能力。
- 垂直搜索与智能问答: 针对特定领域或企业内容的智能搜索和问答机器人，`RAG`天然适合此类场景。用户问题触发对专属知识库的检索，检索到的相关内容被用于生成精准、简洁、符合上下文的答案，提供比传统关键词匹配更自然、信息量更大。

本文实现了非常基础的`RAG`示例，`Embedding`使用轻量的`all-MiniLM-L6-v2`模型，向量检索的数据库使用轻量的`hnswlib-node`实现，以此可以在`node`中直接运行起来 <https://github.com/WindRunnerMax/webpack-simple-environment/tree/master/packages/hnsw-rag>。

实际上当前很多云服务商以及开源项目提供了开箱即用`RAG`服务的实现，而我们自然可以根据需求来决定是否可以接入开箱即用的服务。只是若是我们需要精细地调配很多功能，例如自定义分片策略等，就比较依赖服务是否暴露相关实现，因此我们不一定可以直接处理。

所以在这篇文章中我们实现了简单的`RAG`服务，也可以从侧面反映出来我们能够在`RAG`服务上做些什么精调的方案，来提高召回率和召回效果。在知道了我们可以对服务做哪些方面的参数和数据调整后，才能够比较有针对性地进行优化。

## 文本向量化
先前已经提到了，我们在这里主要的侧重点还是倾向传统的`NLP`检索任务，或者我们可以说是文本的搜索任务。因此简化一下我们需要做的事情就是 预处理-编码-存储-检索 这四部分，在本节中我们主要介绍文本的预处理和编码方式。

### 数据分片
在数据预处理环节，我们主要需要做的是数据清洗以及分片，数据清洗部分主要是去除文本内容中的噪音、重复内容、过滤低信息量文本等等，这部分的策略可以根据具体的业务需求来进行调整。而分片则是将较大的文章切分为较小的片段，以便于后续的向量化和检索。

我们在这里主要讨论的是分片方式，分片的方式解决了两个问题，一是文档的内容会很长，若是直接将文档内容作为整体进行向量化，可能会导致向量的维度过高，计算和存储成本较大。二是文档内容的语义信息可能会分布在不同的段落中，大部分内容可能与用户问题无关，整体向量化后召回效果可能欠佳。

实际上还有个额外的原因，通过这种方式我们可以避免整篇文档被召回，这样可以省去不少的`Token`消耗。并且长上下文给予`LLMs`时，最开始的内容可能还是会被遗忘。因此分片是比较公认的处理方案，分而治之的思想在这里就很合适，并且分片的方式我们可以去精调，来提高召回率以及召回效果。

接下来我们可以考虑分片的方式，常见的分片方式有 固定大小分片、基于句段分片、基于结构分片 等方式。在这里我们以下面的一段文本为例，简单介绍一下分片的方式。需要注意的是，由于我们是为了演示用的，分片切的比较小，而实际上分片通常选的是`512/1024/4096`等长度的片段。

```
## 深度学习
深度学习是机器学习的一个子集，使用多层神经网络来模拟人脑处理信息的方式。它在图像识别、语音识别和自然语言处理等领域取得了显著进展。
```

最基本的分片方式是按照固定的长度分片，即固定一个最大长度，然后根据这个长度值直接切分。这种简单粗暴的方式可以快速实现原型，但可能会导致语义信息的丢失。例如，若我们将上面的文本按照每个分片最大长度为`30`字符进行分片，得到的结果是:

```js
[
  "## 深度学习\n深度学习是机器学习的一个子集，使用多层神经网",
  "络来模拟人脑处理信息的方式。它在图像识别、语音识别和自然语言",
  "处理等领域取得了显著进展。"
]
```

虽然文本看起来非常规整，但实际上语义信息被切割得比较碎片化，而且部分片段可能包含填充或冗余信息，导致检索精度降低。而处理这个问题比较常见的思路是采用`overlap`重叠分片的方式，即在每个分片之间保留一定的重叠部分，以便于保留更多的上下文信息，我们在上述基础上重叠`5`个字符:

```js
[
  "## 深度学习\n深度学习是机器学习的一个子集，使用多层神经网络来模拟人",
  "多层神经网络来模拟人脑处理信息的方式。它在图像识别、语音识别和自然语言处理等领域",
  "和自然语言处理等领域取得了显著进展。"
]
```

虽然看起来是好了不少，但实际上这种方式仍然会导致语义信息的丢失，尤其是当分片长度较短时，可能会导致上下文信息的缺失，而上述提到的分片长度比较大的情况下是表现相对好一些的。在本文的示例中，就是采用的这种简单的分片方式来进行分片的。

```js
/**
 * 文本分片方法
 */
const spiltTextChunks = (text: string): string[] => {
  // 这里是直接根据 固定长度 + overlap 分片，此外还有 结构分片 等策略
  const chunkSize = 100;
  const overlap = 20;
  const chunks: string[] = [];
  for (let i = 0; i < text.length; i += chunkSize - overlap) {
    const chunk = text.slice(i, i + chunkSize);
    if (chunk.trim().length > 0) {
      chunks.push(chunk.trim());
    }
  }
  return chunks;
};
```

第二种分片的方式是基于句段分片，即根据文本中的句子或段落进行分片。这种方式可以更好地保留语义信息，因为这样通常会在句子或段落的边界进行切分，而不是任意长度的字符切分。例如，我们可以将上面的文本按照句子进行分片，得到的结果是:

```js
[
  "## 深度学习",
  "深度学习是机器学习的一个子集，使用多层神经网络来模拟人脑处理信息的方式。",
  "它在图像识别、语音识别和自然语言处理等领域取得了显著进展。"
]
```

这种方式看起来会好很多，语义信息保留得比较完整，避免中间分割，确保片段包含连贯思想，有效维护文档的原始流程和上下文完整性。但是实际上还是面临一些问题，例如切出的句子长度太短，或者是切出的句子太长超出了我们的`Max Token`限制等问题。

因此我们也需要针对其实际表现来额外增加一些策略，例如较短句子中我们可以不断拼接后续的内容，直至其逼近`Max Token`限制。而较长的句子我们可以采用固定分片方式进行分割处理，来确保每个片段都在合理的长度范围内，这样可以更好地平衡语义信息的完整性和片段长度的限制。


```js
[
  "## 深度学习\n深度学习是机器学习的一个子集，使用多层神经网络来模拟人脑处理信息的方式。它在图像识别、语音识别和自然语言处理等领域取得了显著进展。"
]
```

第三种分片方式是基于结构分片，即根据文本的结构进行分片，例如标题、段落、列表等，特别是我们给予模型的输入通常都是`Markdown`格式的文本，因此其天然就具有一定的结构化信息。基于结构分片可以更好地保留文本的层次和逻辑关系，避免语义信息的丢失。

```js
[
  "- 列表1\n- 列表2\n- 列表3",
  "## 深度学习\n深度学习是机器学习的一个子集，使用多层神经网络来模拟人脑处理信息的方式。它在图像识别、语音识别和自然语言处理等领域取得了显著进展。"
]
```

实际上，当前并没有一种普遍适用的最佳分片策略，分片的复杂点在于确定片段的最佳大小和结构。如果片段过小，它们可能会丢失关键的上下文信息，从而变得不那么有意义。相反，如果片段过大，检索过程的效率会降低，并且可能会无意中包含不相关的信息，从而稀释结果的相关性。

除了上述策略，分片时还需考虑其他重要因素，理想情况下，片段应保留语义单元，例如完整的句子、段落或主题，以确保连贯性并防止意义中断。采用重叠片段或者滑动窗口其实还是个比较好的方案，有助于在片段边界处保持上下文，确保关键信息或过渡上下文不会在片段之间丢失。

此外，我们甚至还可以让`LLMs`来帮助我们进行分片，语言模型其实更擅长理解和处理自然语言文本，这也是一种基于语义分片的策略。然而这种方式在处理大量数据的时候就需要考虑效率以及计算资源问题了，而且`LLMs`也并非银弹，具体效果还是需要构建评测集合来验证。

还有一个比较重要的点是，我们在分片的时候是可以并入元信息的，这点是很容易被忽视的。元信息可以是文档的标题、作者、发布时间等，这些信息可以帮助模型更好地理解片段的上下文和背景，甚至如果我们有文档相关的人工打标信息，会更好地帮助我们进行重排以及内容生成。


### 编码方式
为什么要使用向量 utf-8?

## 向量检索

### 向量距离

### 混合召回

### 召回重排

## LLMs

### Query 改写

### 输入优化

### 生成增强

## 总结


如果`LLMs`存在足够长的输入，是否有必要使用`RAG`服务来补充`Context`信息。

`LangChain`


## 每日一题

- <https://github.com/WindRunnerMax/EveryDay>

## 参考
- <https://www.zhihu.com/question/653424464>
- <https://www.volcengine.com/docs/82379/1583857>
- <https://blog.langchain.com/evaluating-rag-pipelines-with-ragas-langsmith/>
